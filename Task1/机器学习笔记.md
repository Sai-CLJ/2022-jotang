# 机器学习笔记

## 1 引入

### 1.1 机器学习的定义

> "A computer program is said to *learn* from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."

也即，如果一个计算机程序可以通过经验E使得其在处理任务T时获得更好的效果（这个效果的好坏用P来衡量，即P为性能指标/性能度量），那么它就可以从经验E中学习关于某个任务T和某个性能指标P的知识。



### 1.2 监督/无监督学习

- 监督学习：对数据库中的每个样本，期望算法预测并得出正确答案

  * 回归(Regression)问题：预测连续值输出(real-valued output)，如房价预测

  * 离散(Classification)问题：预测离散值输出(discrete-valued output)，如良性和恶性肿瘤
- 无监督学习：根据类别未知的训练样本解决模式识别中的各种问题，聚类问题是其代表。



### 1.3 分类器/预测器

* 分类器：利用某些函数将输入样例分为若干类的机器。
* 预测器：根据（学习到的）从输入空间X到输出空间Y的映射，对输入数据进行预测的机器。其输出可以是连续值或离散值。



## 2 基本概念

### 2.1 损失函数、梯度下降

* 损失函数：用于衡量模型所作出的预测离真实值（**Ground Truth**）之间的偏离程度。
  其中回归问题最常用的损失函数为L2损失函数和L1损失函数，二者度量了模型估计值d与观测值θ之间的差异：   ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220911171713.jpg)

  分类问题最常用的则是0-1损失函数：

  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/image-20220906100957131.png)

* 梯度：沿梯度方向，方向导数最大，函数变化率最大，增长速度最快。
* 学习率α：学习率乘以梯度，决定每次沿损失函数下降的步长
* 梯度下降：沿梯度下降的方向递归地求解函数极小值的方法。迭代公式即
   ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/image-20220911172026394.png)





### 2.2 训练集/测试集，过拟合/欠拟合

* 训练集/测试集：
  将数据划分为两个集合，使用训练集的数据来训练模型，再用测试集上的误差来近似表示模型在现实场景中的泛化误差。
  通常将数据集的80%作为训练集，20%作为测试集，也可以采用分层采样等方式划分。

* 过拟合/欠拟合：
  **过拟合**是过度充分地学习了训练基的一些特征，使得损失函数值极小但泛化性能差的情况。也即训练集的损失函数值很小，但是验证集/测试集上的损失函数值很大。
  **欠拟合**则是训练集上的损失函数值就很大，没有充分学习训练集的特征。
  下图中图1是欠拟合的曲线，图3是过拟合的曲线。

  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/v2-b4ccd2c27516e24531f956a85296f6b5_1440w.jpg)









### 2.3 模型性能度量

* **回归任务**最常用的性能度量是均方误差（MSE）

![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220911173928.jpg)

​		以下介绍**分类任务**中常用的性能度量





* 查准率、查全率、F1：
  下面的分类混淆矩阵适用于多分类问题

![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220911174306.jpg)
	定义查准率P和查全率R为
![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220911174423.jpg)

​    可以看出P和R是一对矛盾的度量，因此定义综合考虑了P、R的模型性能度量F1：
![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220911174621.jpg)







* ROC、AUC

  首先定义真正例率TPR和假正例率FPR：
  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912093324.jpg)

  以FPR为横轴，TPR为纵轴建立坐标系，将样本点在系中描点并连线，得到**ROC曲线**。**AUC**是ROC曲线与坐标轴围成的面积。
  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912093646.jpg)
  显然，曲线越接近左上角，分类器的性能就越高，而AUC可以作为性能度量，比较不同分类器性能的判据。



### 2.4 独热编码

* **独热编码**，即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。

> 例如，对六个状态进行编码：
> 自然顺序码为 000,001,010,011,100,101
> 独热编码则是 000001,000010,000100,001000,010000,100000

* 使用独热编码的原因：将离散特征的取值数字化，使离散特征（**无序属性**）可以采用有序属性的距离计算方法（如**闵可夫斯基距离**）。显然，将离散型特征使用独热编码（One-Hot Encoding），会让特征之间的距离计算更加合理。

附：特征向量化后，一种常见的相似性度量——距离计算中，最常用的是闵可夫斯基距离：
![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912100521.jpg)

## 3 神经网络初步



**神经网络**是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。



### 3.1 神经元、激活函数

* M-P神经元模型
  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912103103.jpg)

此模型中，神经元接受来自n个其他神经元传递的输入信号，将总输入值与阈值相比较，再用激活函数处理后产生此神经元的输出值。

+ 激活函数

  + 当不使用激活函数时，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。而激活函数给神经元引入了**非线性因素**，使得神经网络可以任意逼近任何**非线性函数**。


    下面是几种常见的激活函数：

  + 阶跃函数即0-1函数，将输入值映射为0或1，但是不连续、不光滑

  + Sigmoid函数可把输入值挤压到（0，1）内，且非负、光滑。但是它非线性（计算成本高），不以0为中心，同时有梯度消失的问题。

    ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912104225.jpg)

  + Tanh 激活函数，又叫作双曲正切激活函数，解决了Sigmoid不以0为中心的问题
    <img src="https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912104508.jpg" alt="z" style="zoom:67%;" />

  + **ReLU函数**，当输入 x<0 时，输出为 0，当 x> 0 时，输出为 x。该激活函数使网络更快速地收敛。
    同时，在正区域（x> 0 时）可以对抗梯度消失问题，因此神经元至少在一半区域中不会把所有零进行反向传播。
    此外，ReLU 计算效率很高。
    ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912104654.jpg)
    <img src="https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912104924.jpg" alt="s" style="zoom:86%;" />



### 3.2 输入层、隐藏层、输出层、权重

+ 权重：神经元之间传递数据时，均带权传输，最终总输入值为加权求和

* **输入层**神经元接收外界输入，**隐藏层**与**输出层**神经元对信号进行加工，最终结果由**输出层**神经元输出。
  因此输入层仅接收输入，隐层和输出层包含功能神经元。这些功能神经元学习神经元间的“连接权”和每个功能神经元的阈值。

  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912113310.jpg)

  



### 3.3 误差逆传播算法（BP算法）

+ 基本思想为计算输出层误差->将误差逆向传播至隐藏层->根据隐藏层误差迭代连接权和阈值。![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912114525.jpg)
  BP算法仍然基于梯度下降。

+ 由于其表示能力过强，常常发生过拟合，因此可以采取早停或者正则化策略。



### 3.4 卷积神经网络

+ **卷积**
  对函数f和g，其卷积f*g(n)定义为
  ![](https://sai-1312088783.cos.ap-chengdu.myqcloud.com/微信图片_20220912151955.jpg)

> ​    **所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。**

​		直观上看，卷积可以对一维信号或者二维图像等数据进行处理，起到平滑、突出边缘（二维）、模糊背景（二维）等功效。这完全取决于卷积核（滤波器）所设定的值，也即上式中的函数g。

+ **卷积神经网络**（CNN）就是一类包含卷积计算且具有深度结构的前馈神经网络。
  使用它的原因有很多，列举重要的几点：

  + 卷积神经网络具有**表征学习**能力，即能够从输入信息中提取高阶特征。具体地，卷积神经网络中的卷积层和池化层能够响应输入特征的**平移不变性**，即能够识别位于空间不同位置的相近特征。
  + 卷积神经网络中卷积层间的连接被称为**稀疏连接**，即相比于前馈神经网络中的全连接，卷积层中的神经元仅与其相邻层的部分连接，而非与全部神经元相连。这使得网络**不易发生过拟合**，且参数减少使得训练开销降低。
  + 具有生物学相似性，类似视觉神经系统中视觉皮层的组织。

  常见的卷积神经网络由输入层、卷积层，ReLU层、池化（Pooling）层和全连接层构成。
  ——最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。

+ **输入层**：其输入神经元通常是3维排列。以图像为例，输入层的长、宽分别对应图像的长、宽，而输入层的**深度**对应RGB的三通道。

+ **卷积层**——**局部连接**和**权值共享**

  + 卷积层的神经元与输入层局部区域连接，这个局部区域的大小被称为神经元的**感受野**，也即滤波器/卷积核的空间大小。
  + 权值共享的含义是，在卷积过程（即滤波器扫过输入层的每一寸的过程）中，滤波器始终是同一个滤波器。也即，只需学习滤波器空间大小那么大的参数就可以。
    权值共享是基于这样的一个合理的假设：如果一个特征在计算某个空间位置 (x1,y1)(x1,y1) 的时候有用，那么它在计算另一个不同位置 (x2,y2)(x2,y2) 的时候也有用。基于这个假设，可以显著地减少参数数量。
  + 卷积核学习的深度与输入层深度保持一致。仍以图像为例，若卷积图像大小为3x3，那么卷积核大小就应该是3x3x3，后一个3是RGB通道数，也即输入层的深度。
  + 当然，我们不只使用一种卷积核。卷积核种类的个数对应权重集的个数。

+ **ReLU层**或者其他类型的激活层连接在卷积层上，可以使得卷积核在检测到特定特征时才被激活。

+ **池化层**的作用是逐渐降低数据体的空间尺寸，以此就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。
  池化与卷积类似，但是更进一步浓缩了特征。它不会像卷积一样填0，而是用池化大小将卷积层的输出不重合地扫过，并给出一个输出值。这样，在保留深度的同时，使数据量更小。

+ **全连接层**连接在池化层后，与普通的神经网络相同。
